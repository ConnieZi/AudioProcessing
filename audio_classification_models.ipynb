{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Danish this is important for you to run\n",
    "import parselmouth\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "#Danish you can comment this one\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "#import set of functions I made for the automatic analysis of the audios\n",
    "import sys\n",
    "\n",
    "#here the path needs to be changed to your own local path\n",
    "sys.path.insert(1, '/Users/jab464/Documents/Github/EMBRACE-data-analysis/')\n",
    "from audio_analysis_functions import *\n",
    "\n",
    "#Automatic transcription modules needed\n",
    "import whisper\n",
    "from whisper.utils import get_writer\n",
    "\n",
    "#import needed to create a folder\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import unidecode\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summer 23 data sample\n",
    "\n",
    "#Family 03 - mixture of english and spanish (mostly english from the kids side)\n",
    "filename = \"../EMBRACE-data-analysis/data/summer23-pitt/Family_03/03_Chromebook Data/03_voice-recordings/voice-recording-1683505777195-1683505963551.mp3\"\n",
    "\n",
    "#Family 05\n",
    "#filename=\"../EMBRACE-data-analysis/data/summer23-pitt/Family_05/05_Chromebook Data/05_voice-recordings/voice-recording-1686071270169-1686071413625.mp3\"\n",
    "\n",
    "#Family 41 - all spanish\n",
    "# filename =\"../EMBRACE-data-analysis/data/summer23-pitt/Family_41/41_Chromebook Data/41_voice-recordings/voice-recording-1686527367557-1686527501435.mp3\"\n",
    "#\n",
    "filename_no_mp3 = filename[0:len(filename)-4]\n",
    "#\n",
    "# #only the name of the file withouth the whole path\n",
    "filename_no_mp3_no_folder = filename_no_mp3[filename_no_mp3.rfind('/')+1:len(filename_no_mp3)]\n",
    "#\n",
    "# root_folder = filename_no_mp3[0:filename_no_mp3.rfind('/')]\n",
    "\n",
    "# print(filename_no_mp3)\n",
    "# print(filename_no_mp3_no_folder)\n",
    "# print(root_folder)\n",
    "\n",
    "def get_labels(filename):\n",
    "    filename_no_mp3 = filename[0:len(filename)-4]\n",
    "\n",
    "    #only the name of the file withouth the whole path\n",
    "    filename_no_mp3_no_folder = filename_no_mp3[filename_no_mp3.rfind('/')+1:len(filename_no_mp3)]\n",
    "\n",
    "    root_folder = filename_no_mp3[0:filename_no_mp3.rfind('/')]\n",
    "\n",
    "    print(filename_no_mp3)\n",
    "    print(filename_no_mp3_no_folder)\n",
    "    print(root_folder)\n",
    "\n",
    "    labels_list = []\n",
    "\n",
    "    file_labels_danish = find(filename_no_mp3_no_folder+\".mp3_labeled_danish.txt\",\"../EMBRACE-data-analysis/data\")\n",
    "    file_labels_jordan = find(filename_no_mp3_no_folder+\".mp3_labeled_jordan.txt\",\"../EMBRACE-data-analysis/data\")\n",
    "    print(filename_no_mp3_no_folder+\".mp3_labeled_danish.txt\")\n",
    "    print(filename_no_mp3_no_folder+\".mp3_labeled_jordan.txt\")\n",
    "    print(file_labels_danish)\n",
    "    print(file_labels_jordan)\n",
    "\n",
    "    if(file_labels_danish!=None and file_labels_jordan==None):\n",
    "        #file_labels_path = root_folder+\"/\"+filename_no_mp3_no_folder+\".mp3_labeled_danish.txt\"\n",
    "        file_labels_path =  os.path.abspath(file_labels_danish)\n",
    "\n",
    "    else:\n",
    "        file_labels_path =  os.path.abspath(file_labels_jordan)\n",
    "    print(file_labels_path)\n",
    "    file_labels=open(file_labels_path,\"r\")\n",
    "    counter = 0\n",
    "\n",
    "    for line in file_labels:\n",
    "        raw_labels = line.split(\" \")\n",
    "        labels = []\n",
    "        labels.append(filename_no_mp3_no_folder)\n",
    "        labels.append(raw_labels[0])\n",
    "        labels.append(raw_labels[1][1:len(raw_labels[1])-1])\n",
    "        labels.append(raw_labels[2][0:len(raw_labels[2])-1])\n",
    "        labels.append(raw_labels[3][0:len(raw_labels[3])-1])\n",
    "        counter = counter + 1\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    labels_df = df = pd.DataFrame(labels_list, columns =['filename', 'index', 'start','end','label'])\n",
    "    print(labels_df)\n",
    "\n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#extract only the active voice audio\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m pipeline_act_detection \u001B[38;5;241m=\u001B[39m \u001B[43mPipeline\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyannote/voice-activity-detection\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      3\u001B[0m                                         use_auth_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhf_DHDEpmiDLkwrxpSGIdivCjCbkbmqEwdhwx\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#extracted audio\u001B[39;00m\n\u001B[1;32m      5\u001B[0m output \u001B[38;5;241m=\u001B[39m pipeline_act_detection(filename)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "#extract only the active voice audio\n",
    "pipeline_act_detection = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\",\n",
    "                                        use_auth_token=\"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\")\n",
    "#extracted audio\n",
    "output = pipeline_act_detection(filename)\n",
    "\n",
    "# Specify the name of the new folder\n",
    "folder_path = filename_no_mp3+\"_no_silences\"\n",
    "\n",
    "if os.path.exists(folder_path):\n",
    "    print(f\"The path '{folder_path}' exists.\")\n",
    "else:\n",
    "    print(f\"The path '{folder_path}' does not exist.\")\n",
    "    # Create the new folder\n",
    "    os.mkdir(folder_path)\n",
    "\n",
    "try:\n",
    "    audio = AudioSegment.from_file(filename, \"mp3\")\n",
    "except:\n",
    "    audio = AudioSegment.from_file(filename, format=\"mp4\")\n",
    "\n",
    "counter = 0\n",
    "for speech in output.get_timeline().support():\n",
    "    # Extract the chunk\n",
    "    start_time = speech.start\n",
    "    end_time = speech.end\n",
    "    chunk = audio[start_time*1000:end_time*1000]\n",
    "    # Export the extracted chunk to a new audio file\n",
    "    chunk.export(folder_path+\"/\"+filename_no_mp3_no_folder+\"_\"+str(counter)+\".mp3\", format=\"mp3\")\n",
    "    counter=counter+1\n",
    "\n",
    "# List of audio file paths to concatenate\n",
    "audio_files = []\n",
    "\n",
    "#fill audio files list with all the active voice chunks generated in the last for loop\n",
    "for i in range (0,counter):\n",
    "    audio_files.append(folder_path+\"/\"+filename_no_mp3_no_folder+\"_\"+str(i)+\".mp3\")\n",
    "\n",
    "# Initialize an empty AudioSegment object to hold the concatenated audio\n",
    "concatenated_audio = AudioSegment.empty()\n",
    "\n",
    "# Iterate through each audio file\n",
    "for file in audio_files:\n",
    "    # Load the audio file\n",
    "    curr_audio = AudioSegment.from_file(file)\n",
    "\n",
    "    # Append the loaded audio to the concatenated audio\n",
    "    concatenated_audio += curr_audio\n",
    "\n",
    "# Export the concatenated audio withouth silences to a new file\n",
    "concatenated_audio.export(folder_path+\"/\"+filename_no_mp3_no_folder+\"_no_silences.mp3\", format=\"mp3\")\n",
    "\n",
    "#Gets the file without silences (yes again) but in a different format\n",
    "voice_activity = get_voice_activity(filename,False,False)\n",
    "#it segments the data in 1 second segments\n",
    "custom_size_voice_activity = custom_size_segments(voice_activity)\n",
    "\n",
    "#Sound with no silences\n",
    "#snd = parselmouth.Sound(folder_path+\"/\"+filename_no_mp3_no_folder+\"_no_silences.mp3\")\n",
    "\n",
    "snd = parselmouth.Sound(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze a segment and extract MFCC and pitch values\n",
    "def analyze_segment(snd, start_time, end_time):\n",
    "    # Extract a part of the sound between start_time and end_time\n",
    "    segment = snd.extract_part(from_time=start_time, to_time=end_time, preserve_times=True)\n",
    "    segment_duration = segment.xmax - segment.xmin\n",
    "    #print(segment_duration)\n",
    "    pitch_values = []\n",
    "    mfcc_values = []\n",
    "    if(segment_duration>=0.1):\n",
    "        # Extract pitch for the segment\n",
    "        pitch = segment.to_pitch()\n",
    "        pitch_values = pitch.selected_array['frequency']\n",
    "        #print(pitch_values)\n",
    "        # Replace unvoiced (0) with NaN\n",
    "        #pitch_values[pitch_values == 0] = np.nan\n",
    "        # Extract MFCC for the segment\n",
    "        mfcc = segment.to_mfcc()\n",
    "        mfcc_values = mfcc.to_array()\n",
    "        harmonicity = segment.to_harmonicity()\n",
    "        spectrum = segment.to_spectrum()\n",
    "    return pitch_values, mfcc_values, harmonicity, spectrum\n",
    "\n",
    "def add_labeled_data(current_df,filename,labels_df):\n",
    "    filename_no_mp3 = filename[0:len(filename)-4]\n",
    "\n",
    "    #only the name of the file withouth the whole path\n",
    "    filename_no_mp3_no_folder = filename_no_mp3[filename_no_mp3.rfind('/')+1:len(filename_no_mp3)]\n",
    "\n",
    "    root_folder = filename_no_mp3[0:filename_no_mp3.rfind('/')]\n",
    "\n",
    "    #extract only the active voice audio\n",
    "    pipeline_act_detection = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\",\n",
    "                                            use_auth_token=\"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\")\n",
    "    #extracted audio\n",
    "    output = pipeline_act_detection(filename)\n",
    "\n",
    "    # Specify the name of the new folder\n",
    "    folder_path = filename_no_mp3+\"_no_silences\"\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"The path '{folder_path}' exists.\")\n",
    "    else:\n",
    "        print(f\"The path '{folder_path}' does not exist.\")\n",
    "        # Create the new folder\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(filename, \"mp3\")\n",
    "    except:\n",
    "        audio = AudioSegment.from_file(filename, format=\"mp4\")\n",
    "\n",
    "    counter = 0\n",
    "    for speech in output.get_timeline().support():\n",
    "        # Extract the chunk\n",
    "        start_time = speech.start\n",
    "        end_time = speech.end\n",
    "        chunk = audio[start_time*1000:end_time*1000]\n",
    "        # Export the extracted chunk to a new audio file\n",
    "        chunk.export(folder_path+\"/\"+filename_no_mp3_no_folder+\"_\"+str(counter)+\".mp3\", format=\"mp3\")\n",
    "        counter=counter+1\n",
    "\n",
    "    # List of audio file paths to concatenate\n",
    "    audio_files = []\n",
    "\n",
    "    #fill audio files list with all the active voice chunks generated in the last for loop\n",
    "    for i in range (0,counter):\n",
    "        audio_files.append(folder_path+\"/\"+filename_no_mp3_no_folder+\"_\"+str(i)+\".mp3\")\n",
    "\n",
    "    # Initialize an empty AudioSegment object to hold the concatenated audio\n",
    "    concatenated_audio = AudioSegment.empty()\n",
    "\n",
    "    # Iterate through each audio file\n",
    "    for file in audio_files:\n",
    "        # Load the audio file\n",
    "        curr_audio = AudioSegment.from_file(file)\n",
    "\n",
    "        # Append the loaded audio to the concatenated audio\n",
    "        concatenated_audio += curr_audio\n",
    "\n",
    "    # Export the concatenated audio withouth silences to a new file\n",
    "    concatenated_audio.export(folder_path+\"/\"+filename_no_mp3_no_folder+\"_no_silences.mp3\", format=\"mp3\")\n",
    "\n",
    "    #Gets the file without silences (yes again) but in a different format\n",
    "    voice_activity = get_voice_activity(filename,False,False)\n",
    "    #it segments the data in 1 second segments\n",
    "    custom_size_voice_activity = custom_size_segments(voice_activity)\n",
    "\n",
    "    #Sound with no silences\n",
    "    #snd = parselmouth.Sound(folder_path+\"/\"+filename_no_mp3_no_folder+\"_no_silences.mp3\")\n",
    "\n",
    "    snd = parselmouth.Sound(filename)\n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    max_pitch_length = 97  # Maximum number of pitch values per segment\n",
    "    num_mfcc = 13  # Assuming 13 MFCC coefficients for simplicity\n",
    "\n",
    "    labels = [f\"Label_{i+1}\" for i in range(len(custom_size_voice_activity))]\n",
    "\n",
    "    for i, segment in enumerate(custom_size_voice_activity):\n",
    "        start, end = segment\n",
    "\n",
    "        #Sets metadata (filename,index,start,end,duration)\n",
    "        metadata = []\n",
    "        metadata.append(filename_no_mp3_no_folder)\n",
    "        metadata.append(i)\n",
    "        metadata.append(start)\n",
    "        metadata.append(end)\n",
    "        duration_segment = float(end)-float(start)\n",
    "        metadata.append(duration_segment)\n",
    "\n",
    "        print(duration_segment)\n",
    "\n",
    "        if(duration_segment>=.1):\n",
    "            #find label from the labeled dataset\n",
    "            label = labels_df.at[i,'label']\n",
    "            print(label)\n",
    "            labels[i]=label\n",
    "\n",
    "            pitch_values, mfcc_values, harmonicity, spectrum = analyze_segment(snd, start, end)\n",
    "            # Calculate the average pitch, excluding NaN values\n",
    "            avg_pitch = np.nanmean(pitch_values)\n",
    "            if(len(pitch_values)<97):\n",
    "                last_index = len(pitch_values)\n",
    "                for j in range (len(pitch_values),97):\n",
    "                    pitch_values=np.append(pitch_values,np.nan)\n",
    "            # getting the avg mfcc\n",
    "            mfcc_avg = np.mean(mfcc_values, axis=1)\n",
    "            #adds data corresponding to harmonicity\n",
    "            harmonicity_values = harmonicity.values[0]\n",
    "            #print(harmonicity_values)\n",
    "            harmonicity_values_filtered = [x for x in harmonicity_values if x > -200]\n",
    "            #harmonicity_values_filtered = [harmonicity_values > -200]\n",
    "            median_harmonicity = np.nan\n",
    "            max_harmonicity=np.nan\n",
    "            min_harmonicity=np.nan\n",
    "            avg_harmonicity=np.nan\n",
    "            sd_harmonicity=np.nan\n",
    "            if(len(harmonicity_values_filtered)>0):\n",
    "                median_harmonicity=statistics.median(harmonicity_values_filtered)\n",
    "                max_harmonicity=np.max(harmonicity_values_filtered)\n",
    "                min_harmonicity=np.min(harmonicity_values_filtered)\n",
    "                avg_harmonicity=statistics.mean(harmonicity_values_filtered)\n",
    "                sd_harmonicity=np.std(harmonicity_values_filtered,dtype=np.float64)\n",
    "            harmonicity_list=[median_harmonicity,max_harmonicity,min_harmonicity,avg_harmonicity,sd_harmonicity]\n",
    "\n",
    "            #TO_DO: we should add data related to the spectrum of the audio data\n",
    "            spectrum_list = []\n",
    "            spectrum_band_density = spectrum.get_band_density()\n",
    "            spectrum_band_energy = spectrum.get_band_energy()\n",
    "            spectrum_gravity_center = spectrum.get_center_of_gravity()\n",
    "            spectrum_hf = spectrum.get_highest_frequency()\n",
    "            spectrum_lf = spectrum.get_lowest_frequency()\n",
    "            spectrum_sd = spectrum.get_standard_deviation()\n",
    "            spectrum_kurtosis = spectrum.get_kurtosis()\n",
    "            spectrum_skewness = spectrum.get_skewness()\n",
    "            spectrum_list = [spectrum_band_density,spectrum_band_energy,spectrum_gravity_center,spectrum_hf,spectrum_lf,spectrum_sd,spectrum_kurtosis,spectrum_skewness]\n",
    "\n",
    "            # Combine all the data for this segment into one list\n",
    "            segment_data = list(metadata) + list(pitch_values) + [avg_pitch] + list(mfcc_avg) + harmonicity_list + spectrum_list + [labels[i]]\n",
    "\n",
    "            # Append the segment's data to the overall data list\n",
    "            data.append(segment_data)\n",
    "\n",
    "\n",
    "    # Column names: 97 pitch columns, 1 avg pitch column, MFCC columns, and 1 label column\n",
    "    column_names = ['filename','index','start','end','duration'] + [f'Pitch_{i+1}' for i in range(max_pitch_length)] + ['Avg_Pitch'] + [f'MFCC_{j+1}' for j in range(num_mfcc)] + ['Median_harmonicity','Max_harmonicity','Min_harmonicity','Avg_harmonicity','Sd_harmonicity','Spectrum_density','Spectrum_energy','Spectrum_gravity_center','Spectrum_hf','Spectrum_lf','Spectrum_sd','Spectrum_kurtosis','Spectrum_skewness','Label']\n",
    "\n",
    "    # Create a DataFrame from the data list\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    #Concatenate that df to the global df\n",
    "    current_df = pd.concat([current_df, df])\n",
    "    current_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return current_df\n",
    "\n",
    "def add_labeled_data_no_labels(current_df,filename):\n",
    "    filename_no_mp3 = filename[0:len(filename)-4]\n",
    "\n",
    "    #only the name of the file withouth the whole path\n",
    "    filename_no_mp3_no_folder = filename_no_mp3[filename_no_mp3.rfind('/')+1:len(filename_no_mp3)]\n",
    "\n",
    "    root_folder = filename_no_mp3[0:filename_no_mp3.rfind('/')]\n",
    "\n",
    "    #extract only the active voice audio\n",
    "    pipeline_act_detection = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\",\n",
    "                                            use_auth_token=\"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\")\n",
    "    #extracted audio\n",
    "    output = pipeline_act_detection(filename)\n",
    "\n",
    "    # Specify the name of the new folder\n",
    "    folder_path = filename_no_mp3+\"_no_silences\"\n",
    "\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"The path '{folder_path}' exists.\")\n",
    "    else:\n",
    "        print(f\"The path '{folder_path}' does not exist.\")\n",
    "        # Create the new folder\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(filename, \"mp3\")\n",
    "    except:\n",
    "        audio = AudioSegment.from_file(filename, format=\"mp4\")\n",
    "\n",
    "    counter = 0\n",
    "    for speech in output.get_timeline().support():\n",
    "        # Extract the chunk\n",
    "        start_time = speech.start\n",
    "        end_time = speech.end\n",
    "        chunk = audio[start_time*1000:end_time*1000]\n",
    "        # Export the extracted chunk to a new audio file\n",
    "        chunk.export(folder_path+\"/\"+filename_no_mp3_no_folder+\"_\"+str(counter)+\".mp3\", format=\"mp3\")\n",
    "        counter=counter+1\n",
    "\n",
    "    # List of audio file paths to concatenate\n",
    "    audio_files = []\n",
    "\n",
    "    #fill audio files list with all the active voice chunks generated in the last for loop\n",
    "    for i in range (0,counter):\n",
    "        audio_files.append(folder_path+\"/\"+filename_no_mp3_no_folder+\"_\"+str(i)+\".mp3\")\n",
    "\n",
    "    # Initialize an empty AudioSegment object to hold the concatenated audio\n",
    "    concatenated_audio = AudioSegment.empty()\n",
    "\n",
    "    # Iterate through each audio file\n",
    "    for file in audio_files:\n",
    "        # Load the audio file\n",
    "        curr_audio = AudioSegment.from_file(file)\n",
    "\n",
    "        # Append the loaded audio to the concatenated audio\n",
    "        concatenated_audio += curr_audio\n",
    "\n",
    "    # Export the concatenated audio withouth silences to a new file\n",
    "    concatenated_audio.export(folder_path+\"/\"+filename_no_mp3_no_folder+\"_no_silences.mp3\", format=\"mp3\")\n",
    "\n",
    "    #Gets the file without silences (yes again) but in a different format\n",
    "    voice_activity = get_voice_activity(filename,False,False)\n",
    "    #it segments the data in 1 second segments\n",
    "    custom_size_voice_activity = custom_size_segments(voice_activity)\n",
    "\n",
    "    #Sound with no silences\n",
    "    #snd = parselmouth.Sound(folder_path+\"/\"+filename_no_mp3_no_folder+\"_no_silences.mp3\")\n",
    "\n",
    "    snd = parselmouth.Sound(filename)\n",
    "    # Initialize lists to store data\n",
    "    data = []\n",
    "    max_pitch_length = 97  # Maximum number of pitch values per segment\n",
    "    num_mfcc = 13  # Assuming 13 MFCC coefficients for simplicity\n",
    "\n",
    "    #labels = [f\"Label_{i+1}\" for i in range(len(custom_size_voice_activity))]\n",
    "\n",
    "    for i, segment in enumerate(custom_size_voice_activity):\n",
    "        start, end = segment\n",
    "\n",
    "        #Sets metadata (filename,index,start,end,duration)\n",
    "        metadata = []\n",
    "        metadata.append(filename_no_mp3_no_folder)\n",
    "        metadata.append(i)\n",
    "        metadata.append(start)\n",
    "        metadata.append(end)\n",
    "        duration_segment = float(end)-float(start)\n",
    "        metadata.append(duration_segment)\n",
    "\n",
    "        print(duration_segment)\n",
    "\n",
    "        if(duration_segment>=.1):\n",
    "            #find label from the labeled dataset\n",
    "            #label = labels_df.at[i,'label']\n",
    "            #print(label)\n",
    "            #labels[i]=label\n",
    "\n",
    "            pitch_values, mfcc_values, harmonicity, spectrum = analyze_segment(snd, start, end)\n",
    "            # Calculate the average pitch, excluding NaN values\n",
    "            avg_pitch = np.nanmean(pitch_values)\n",
    "            if(len(pitch_values)<97):\n",
    "                last_index = len(pitch_values)\n",
    "                for j in range (len(pitch_values),97):\n",
    "                    pitch_values=np.append(pitch_values,np.nan)\n",
    "            # getting the avg mfcc\n",
    "            mfcc_avg = np.mean(mfcc_values, axis=1)\n",
    "            #adds data corresponding to harmonicity\n",
    "            harmonicity_values = harmonicity.values[0]\n",
    "            #print(harmonicity_values)\n",
    "            harmonicity_values_filtered = [x for x in harmonicity_values if x > -200]\n",
    "            #harmonicity_values_filtered = [harmonicity_values > -200]\n",
    "            median_harmonicity = np.nan\n",
    "            max_harmonicity=np.nan\n",
    "            min_harmonicity=np.nan\n",
    "            avg_harmonicity=np.nan\n",
    "            sd_harmonicity=np.nan\n",
    "            if(len(harmonicity_values_filtered)>0):\n",
    "                median_harmonicity=statistics.median(harmonicity_values_filtered)\n",
    "                max_harmonicity=np.max(harmonicity_values_filtered)\n",
    "                min_harmonicity=np.min(harmonicity_values_filtered)\n",
    "                avg_harmonicity=statistics.mean(harmonicity_values_filtered)\n",
    "                sd_harmonicity=np.std(harmonicity_values_filtered,dtype=np.float64)\n",
    "            harmonicity_list=[median_harmonicity,max_harmonicity,min_harmonicity,avg_harmonicity,sd_harmonicity]\n",
    "\n",
    "            #TO_DO: we should add data related to the spectrum of the audio data\n",
    "            spectrum_list = []\n",
    "            spectrum_band_density = spectrum.get_band_density()\n",
    "            spectrum_band_energy = spectrum.get_band_energy()\n",
    "            spectrum_gravity_center = spectrum.get_center_of_gravity()\n",
    "            spectrum_hf = spectrum.get_highest_frequency()\n",
    "            spectrum_lf = spectrum.get_lowest_frequency()\n",
    "            spectrum_sd = spectrum.get_stanhdard_deviation()\n",
    "            spectrum_kurtosis = spectrum.get_kurtosis()\n",
    "            spectrum_skewness = spectrum.get_skewness()\n",
    "            spectrum_list = [spectrum_band_density, spectrum_band_energy, spectrum_gravity_center, spectrum_hf, spectrum_lf, spectrum_sd, spectrum_kurtosis, spectrum_skewness]\n",
    "\n",
    "            # Combine all the data for this segment into one list\n",
    "            segment_data = list(metadata) + list(pitch_values) + [avg_pitch] + list(mfcc_avg) + harmonicity_list + spectrum_list\n",
    "\n",
    "            # Append the segment's data to the overall data list\n",
    "            data.append(segment_data)\n",
    "\n",
    "\n",
    "    # Column names: 97 pitch columns, 1 avg pitch column, MFCC columns, Harmonicity columns, Spectrum columns and 1 label column\n",
    "    column_names = ['filename','index','start','end','duration'] + [f'Pitch_{i+1}' for i in range(max_pitch_length)] + ['Avg_Pitch'] + [f'MFCC_{j+1}' for j in range(num_mfcc)] + ['Median_harmonicity','Max_harmonicity','Min_harmonicity','Avg_harmonicity','Sd_harmonicity','Spectrum_density','Spectrum_energy','Spectrum_gravity_center','Spectrum_hf','Spectrum_lf','Spectrum_sd','Spectrum_kurtosis','Spectrum_skewness']\n",
    "\n",
    "    # Create a DataFrame from the data list\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    #Concatenate that df to the global df\n",
    "    current_df = pd.concat([current_df, df])\n",
    "    current_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return current_df\n",
    "\n",
    "#function that adds summary statistics for pitch to the dataframe\n",
    "def add_pitch_summary_stats(X,min_pitch,max_pitch):\n",
    "    #get only numerical values\n",
    "    #X = X.drop(['index','start','end','duration'], axis=1)\n",
    "    X_pitch = X.loc[:, ~X.columns.isin(['index','start','end','duration',\"filename\",\"Avg_Pitch\",\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\",\"Max_harmonicity\",\"Min_harmonicity\",\"Median_harmonicity\",\"Avg_harmonicity\",\"Sd_harmonicity\",'Spectrum_density','Spectrum_energy','Spectrum_gravity_center','Spectrum_hf','Spectrum_lf','Spectrum_sd','Spectrum_kurtosis','Spectrum_skewness'])]\n",
    "    #add summary stats for pitch\n",
    "    #summary statistics for pitch\n",
    "    #X['Pitch_mean'] = X_pitch.mean(axis=1)\n",
    "    #X['Pitch_sd'] = X_pitch.std(axis=1)\n",
    "    #X['Pitch_median']=X_pitch.median(axis=1)\n",
    "    X['Pitch_max']=X_pitch.max(axis=1)\n",
    "    X['Pitch_min']=X_pitch.min(axis=1)\n",
    "    print(X['Pitch_max'])\n",
    "    X['Pitch_range']=X['Pitch_max']-X['Pitch_min']\n",
    "    #X['Pitch_q1']=X_pitch.quantile(0.25,axis=1)\n",
    "    #X['Pitch_q3']=X_pitch.quantile(0.75,axis=1)\n",
    "    X['Pitch_skewness']=X_pitch.skew(axis=1)\n",
    "    X['Pitch_kurtosis']=X_pitch.kurt(axis=1)\n",
    "    # Apply the function row-wise:mode\n",
    "    modes_per_row = X_pitch.round().astype(int).apply(lambda row: row_mode_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "    X['Pitch_mode']=modes_per_row\n",
    "    # Apply the function row-wise:mean\n",
    "    means_per_row = X_pitch.apply(lambda row: row_mean_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "    X['Pitch_mean']=means_per_row\n",
    "    # Apply the function row-wise:sd\n",
    "    sds_per_row = X_pitch.apply(lambda row: row_sd_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "    X['Pitch_sd']=sds_per_row\n",
    "    # Apply the function row-wise:median\n",
    "    medians_per_row = X_pitch.apply(lambda row: row_median_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "    X['Pitch_median']=medians_per_row\n",
    "    # Apply the function row-wise:median\n",
    "    q1_per_row = X_pitch.apply(lambda row: row_quantile_within_range(row,0.25, min_pitch, max_pitch), axis=1)\n",
    "    X['Pitch_q1']=q1_per_row\n",
    "    # Apply the function row-wise:median\n",
    "    q3_per_row = X_pitch.apply(lambda row: row_quantile_within_range(row,0.75, min_pitch, max_pitch), axis=1)\n",
    "    X['Pitch_q3']=q3_per_row\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: Harmonicity\n",
      "Object name: <no name>\n",
      "Date: Thu Mar 21 12:10:10 2024\n",
      "\n",
      "Time domain:\n",
      "   Start time: 146.2734375 seconds\n",
      "   End time: 146.51718750000003 seconds\n",
      "   Total duration: 0.2437500000000341 seconds\n",
      "Time sampling:\n",
      "   Number of frames: 22 (0 sounding)\n",
      "   Time step: 0.01 seconds\n",
      "   First frame centred at: 146.29031249999997 seconds\n",
      "\n",
      "[]\n",
      "Median: nan\n",
      "Maximum: -200.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMedian: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(median_harmonicity))\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaximum: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(test_harmonicity\u001B[38;5;241m.\u001B[39mget_maximum()))\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMinimum: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mharmonicity_values_filtered\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAverage: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(statistics\u001B[38;5;241m.\u001B[39mmean(harmonicity_values_filtered)))\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSd: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(np\u001B[38;5;241m.\u001B[39mstd(harmonicity_values_filtered,dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat64)))\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mamin\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2918\u001B[0m, in \u001B[0;36mamin\u001B[0;34m(a, axis, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2802\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_amin_dispatcher)\n\u001B[1;32m   2803\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mamin\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[1;32m   2804\u001B[0m          where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n\u001B[1;32m   2805\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2806\u001B[0m \u001B[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001B[39;00m\n\u001B[1;32m   2807\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2916\u001B[0m \u001B[38;5;124;03m    6\u001B[39;00m\n\u001B[1;32m   2917\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmin\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2919\u001B[0m \u001B[43m                          \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "\n",
    "#Testing Parselmouth\n",
    "#test_filename =\"../EMBRACE-data-analysis/data/summer23-pitt/Family_41/41_Chromebook Data/41_voice-recordings/voice-recording-1686527367557-1686527501435.mp3\"\n",
    "test_filename= \"../EMBRACE-data-analysis/data/summer23-pitt/Family_03/03_Chromebook Data/03_voice-recordings/voice-recording-1683505777195-1683505963551_no_silences/voice-recording-1683505777195-1683505963551_no_silences.mp3\"\n",
    "\n",
    "test_sound = parselmouth.Sound(test_filename)\n",
    "\n",
    "start_time=146.2734375\n",
    "end_time=146.51718750000003\n",
    "test_segment = test_sound.extract_part(from_time=start_time, to_time=end_time, preserve_times=True)\n",
    "test_segment_duration = test_segment.xmax - test_segment.xmin\n",
    "#print(segment_duration)\n",
    "test_harmonicity_values = []\n",
    "if(test_segment_duration>=0.1):\n",
    "    test_harmonicity = test_segment.to_harmonicity()\n",
    "    print(test_harmonicity)\n",
    "    harmonicity_values = test_harmonicity.values[0]\n",
    "    harmonicity_values_filtered = [x for x in harmonicity_values if x > -200]\n",
    "    print(harmonicity_values_filtered)\n",
    "    #harmonicity_values_filtered = [harmonicity_values > -200]\n",
    "    #print(statistics.mean(harmonicity_values_filtered))\n",
    "    median_harmonicity = np.nan\n",
    "    if(len(harmonicity_values_filtered)>0):\n",
    "        median_harmonicity = statistics.median(harmonicity_values_filtered)\n",
    "    print(\"Median: \"+str(median_harmonicity))\n",
    "    print(\"Maximum: \"+str(test_harmonicity.get_maximum()))\n",
    "    print(\"Minimum: \"+str(np.min(harmonicity_values_filtered)))\n",
    "    print(\"Average: \"+str(statistics.mean(harmonicity_values_filtered)))\n",
    "    print(\"Sd: \"+str(np.std(harmonicity_values_filtered,dtype=np.float64)))\n",
    "\n",
    "    # test_spectrum = test_segment.to_spectrum()\n",
    "    # print(test_spectrum)\n",
    "    #print(test_spectrum.get_center_of_gravity())\n",
    "    #test_harmonicity_values = test_harmonicity.to_array()\n",
    "#print(test_harmonicity_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m lines \u001B[38;5;241m=\u001B[39m all_labeled_files\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[1;32m      4\u001B[0m last \u001B[38;5;241m=\u001B[39m lines[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m----> 6\u001B[0m current_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mDataFrame()\n\u001B[1;32m      7\u001B[0m counter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "all_labeled_files = open(\"/Users/jab464/Documents/GitHub/EMBRACE-data-analysis/data/pitt_2023_audios.txt\",\"r\")\n",
    "\n",
    "lines = all_labeled_files.readlines()\n",
    "last = lines[-1]\n",
    "\n",
    "current_df = pd.DataFrame()\n",
    "counter = 0\n",
    "for line in lines:\n",
    "    if line is last:\n",
    "        path_labeled_file = line[0:len(line)]\n",
    "        # do work on lst line\n",
    "    else:\n",
    "        # work on other lines\n",
    "        path_labeled_file = line[0:len(line)-1]\n",
    "    print(path_labeled_file)\n",
    "    labels_df = get_labels(path_labeled_file)\n",
    "    current_df = add_labeled_data(current_df,path_labeled_file,labels_df)\n",
    "    counter = counter + 1\n",
    "print(\"labeled files \"+str(counter))\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "current_df.to_csv('audio_features_with_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        filename  index     start       end  duration  \\\n",
      "0  voice-recording-1683119416725      0  0.497812  1.358437  0.860625   \n",
      "1  voice-recording-1683119416725      1  1.645312  2.645313  1.000000   \n",
      "2  voice-recording-1683119416725      2  2.645313  3.645313  1.000000   \n",
      "3  voice-recording-1683119416725      3  3.645313  4.645313  1.000000   \n",
      "4  voice-recording-1683119416725      4  4.645313  5.645313  1.000000   \n",
      "\n",
      "        age    female      male     child pred_label  ...  Spectrum_density  \\\n",
      "0  0.032438  0.003587  0.000359  0.996054          c  ...      1.890000e-07   \n",
      "1  0.568904  0.992115  0.007797  0.000089         af  ...      2.260000e-07   \n",
      "2  0.525433  0.994414  0.003931  0.001655         af  ...      5.060000e-08   \n",
      "3  0.468566  0.989258  0.010635  0.000108         af  ...      3.160000e-07   \n",
      "4  0.320262  0.988031  0.009890  0.002078         af  ...      3.140000e-07   \n",
      "\n",
      "   Spectrum_energy  Spectrum_gravity_center  Spectrum_hf  Spectrum_lf  \\\n",
      "0         0.004543              3478.972525        24000            0   \n",
      "1         0.005425              1773.801785        24000            0   \n",
      "2         0.001215               673.608330        24000            0   \n",
      "3         0.007579               623.100916        24000            0   \n",
      "4         0.007547              1120.455705        24000            0   \n",
      "\n",
      "   Spectrum_sd  Spectrum_kurtosis  Spectrum_skewness  Label  silence_before  \n",
      "0  6074.415936           1.335383           1.791171      c           False  \n",
      "1  4124.106703           9.169199           3.210090     af            True  \n",
      "2  1446.234970          57.393507           6.489962     af           False  \n",
      "3  1233.863694         148.014842          11.302661     af           False  \n",
      "4  2955.603056          23.038988           4.825264     af           False  \n",
      "\n",
      "[5 rows x 134 columns]\n",
      "Number of valid segments 3016\n",
      "Number of adult segments 1855\n",
      "Number of child segments 571\n",
      "Number of simultaneous talk segments 590\n",
      "['Pitch_3', 'Pitch_4', 'Pitch_5', 'Pitch_6', 'Pitch_7', 'Pitch_8', 'Pitch_9', 'Pitch_10', 'Pitch_11', 'Pitch_12', 'Pitch_13', 'Pitch_14', 'Pitch_15', 'Pitch_16', 'Pitch_17', 'Pitch_18', 'Pitch_19', 'Pitch_20', 'Pitch_21', 'Pitch_22', 'Pitch_23', 'Pitch_24', 'Pitch_25', 'Pitch_26', 'Pitch_27', 'Pitch_28', 'Pitch_29', 'Pitch_30', 'Pitch_31', 'Pitch_32', 'Pitch_33', 'Pitch_34', 'Pitch_35', 'Pitch_36', 'Pitch_37', 'Pitch_38', 'Pitch_39', 'Pitch_40', 'Pitch_41', 'Pitch_42', 'Pitch_43', 'Pitch_44', 'Pitch_45', 'Pitch_46', 'Pitch_47', 'Pitch_48', 'Pitch_49', 'Pitch_50', 'Pitch_51', 'Pitch_52', 'Pitch_53', 'Pitch_54', 'Pitch_55', 'Pitch_56', 'Pitch_57', 'Pitch_58', 'Pitch_59', 'Pitch_60', 'Pitch_61', 'Pitch_62', 'Pitch_63', 'Pitch_64', 'Pitch_65', 'Pitch_66', 'Pitch_67', 'Pitch_68', 'Pitch_69', 'Pitch_70', 'Pitch_71', 'Pitch_72', 'Pitch_73', 'Pitch_74', 'Pitch_75', 'Pitch_76', 'Pitch_77', 'Pitch_78', 'Pitch_79', 'Pitch_80', 'Pitch_81', 'Pitch_82', 'Pitch_83', 'Pitch_84', 'Pitch_85', 'Pitch_86', 'Pitch_87', 'Pitch_88', 'Pitch_89', 'Pitch_90', 'Pitch_91', 'Pitch_92', 'Pitch_93', 'Pitch_94', 'Pitch_95', 'Pitch_96', 'Pitch_97']\n",
      "[433, 833, 1177]\n"
     ]
    }
   ],
   "source": [
    "#Once the features have been extracted this part is important for you to run\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Function to calculate mode for a row, excluding values outside a range\n",
    "def row_mode_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.mode()[0] if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate mean for a row, excluding values outside a range\n",
    "def row_mean_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.mean() if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate min for a row, excluding values outside a range\n",
    "def row_min_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.min() if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate max for a row, excluding values outside a range\n",
    "def row_max_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.max() if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate sd for a row, excluding values outside a range\n",
    "def row_sd_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.std() if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate median for a row, excluding values outside a range\n",
    "def row_median_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.median() if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate median for a row, excluding values outside a range\n",
    "def row_quantile_within_range(row, quantile, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.quantile(quantile) if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate skewness for a row, excluding values outside a range\n",
    "def row_skweness_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.skew() if not filtered_row.empty else None\n",
    "\n",
    "# Function to calculate skewness for a row, excluding values outside a range\n",
    "def row_kurtosis_within_range(row, min_value, max_value):\n",
    "    # Exclude values outside the range\n",
    "    filtered_row = row[(row >= min_value) & (row <= max_value)]\n",
    "    return filtered_row.kurt() if not filtered_row.empty else None\n",
    "\n",
    "# Example usage of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# Replace 'your_dataset.csv' with the path to your dataset\n",
    "df = pd.read_csv('/Users/jab464/Documents/GitHub/EMBRACE-data-analysis/audio_features_with_labels.csv')\n",
    "\n",
    "\n",
    "#augment dataset with silence presence before the segment\n",
    "df['silence_before']= df.start > df.end.shift()\n",
    "\n",
    "print(df.head(5))\n",
    "\n",
    "#Filter out where it was just noise\n",
    "# --- not this -> there were 2 talking at the same or there were two speaker talking one after another\n",
    "df = df[(df.Label!=\"-\")]\n",
    "#df = df.dropna()\n",
    "\n",
    "#here we binarize our labels\n",
    "df['binary_label']='a'\n",
    "df.loc[df['Label'].str.contains('c'), 'binary_label'] = 'c'\n",
    "df.loc[df['Label'].str.contains('&'), 'binary_label'] = 'm'\n",
    "df.loc[df['Label'].str.contains(','), 'binary_label'] = 'm'\n",
    "\n",
    "\n",
    "#remove rows with nan values\n",
    "#df = df.dropna()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#remove rows with duration less than 1 sec\n",
    "df = df[df['duration']==1.0]\n",
    "\n",
    "print(\"Number of valid segments \"+str(len(df)))\n",
    "print(\"Number of adult segments \"+str(len(df[df.binary_label=='a'])))\n",
    "print(\"Number of child segments \"+str(len(df[df.binary_label=='c'])))\n",
    "print(\"Number of simultaneous talk segments \"+str(len(df[df.binary_label=='m'])))\n",
    "\n",
    "\n",
    "# Assuming your dataset has features X and target variable y\n",
    "#X = df.drop(['filename','index','start','end','duration','Label','binary_label'], axis=1)\n",
    "#We need to keep filename as it gives info about the family\n",
    "X = df.drop(['index','start','end','duration','Label','binary_label','pred_label'], axis=1)\n",
    "y = df['binary_label']\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y=y_encoded\n",
    "\n",
    "#Getting only pitch values\n",
    "pitch_columns = [col for col in df.columns if col.startswith(\"Pitch_\")]\n",
    "#X_pitch = X.loc[:, ~X.columns.isin([\"filename\",\"Avg_Pitch\",\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\",\"Max_harmonicity\",\"Min_harmonicity\",\"Median_harmonicity\",\"Avg_harmonicity\",\"Sd_harmonicity\"])]\n",
    "print(pitch_columns)\n",
    "X_pitch = X.loc[:,X.columns.isin(pitch_columns)]\n",
    "\n",
    "#Fill na with -1\n",
    "#X_pitch = X_pitch.fillna(-1)\n",
    "\n",
    "#summary statistics for pitch\n",
    "min_pitch=50\n",
    "max_pitch=600\n",
    "\n",
    "#X['Pitch_mean'] = X_pitch.mean(axis=1)\n",
    "#X['Pitch_sd'] = X_pitch.std(axis=1)\n",
    "#X['Pitch_median']=X_pitch.median(axis=1)\n",
    "#X['Pitch_max']=X_pitch.max(axis=1)\n",
    "#Apply the function row-wise:max\n",
    "X['Pitch_max']= X_pitch.apply(lambda row: row_max_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "#X['Pitch_min']=X_pitch.min(axis=1)\n",
    "X['Pitch_min']= X_pitch.apply(lambda row: row_min_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "X['Pitch_range']=X['Pitch_max']-X['Pitch_min']\n",
    "#X['Pitch_q1']=X_pitch.quantile(0.25,axis=1)\n",
    "#X['Pitch_q3']=X_pitch.quantile(0.75,axis=1)\n",
    "#X['Pitch_skewness']=X_pitch.skew(axis=1)\n",
    "#X['Pitch_kurtosis']=X_pitch.kurt(axis=1)\n",
    "\n",
    "X['Pitch_kurtosis']=X_pitch.apply(lambda row: row_kurtosis_within_range(row, min_value=min_pitch, max_value=max_pitch), axis=1)\n",
    "X['Pitch_skewness']=X_pitch.apply(lambda row: row_skweness_within_range(row, min_value=min_pitch, max_value=max_pitch), axis=1)\n",
    "\n",
    "# Apply the function row-wise:mode\n",
    "#modes_per_row = X_pitch.round().astype(int).apply(lambda row: row_mode_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "# Apply the function row-wise:mean\n",
    "means_per_row = X_pitch.apply(lambda row: row_mean_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "X['Pitch_mean']=means_per_row\n",
    "# Apply the function row-wise:sd\n",
    "sds_per_row = X_pitch.apply(lambda row: row_sd_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "X['Pitch_sd']=sds_per_row\n",
    "# Apply the function row-wise:median\n",
    "medians_per_row = X_pitch.apply(lambda row: row_median_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "X['Pitch_median']=medians_per_row\n",
    "# Apply the function row-wise:median\n",
    "q1_per_row = X_pitch.apply(lambda row: row_quantile_within_range(row,0.25, min_pitch, max_pitch), axis=1)\n",
    "X['Pitch_q1']=q1_per_row\n",
    "# Apply the function row-wise:median\n",
    "q3_per_row = X_pitch.apply(lambda row: row_quantile_within_range(row,0.75, min_pitch, max_pitch), axis=1)\n",
    "X['Pitch_q3']=q3_per_row\n",
    "#Mode\n",
    "modes_per_row = X_pitch.round().astype('Int64').apply(lambda row: row_mode_within_range(row, min_pitch, max_pitch), axis=1)\n",
    "\n",
    "X['Pitch_mode']=modes_per_row\n",
    "\n",
    "#Remove nan values from both X and y\n",
    "X_rows_with_nan = X[X.isna().any(axis=1)].index.to_list()\n",
    "print(X_rows_with_nan)\n",
    "y = np.delete(y,X_rows_with_nan)\n",
    "X = X.dropna()\n",
    "\n",
    "X_with_filename = X\n",
    "X= X.drop(['filename'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Combine majority class and oversampled minority class\u001B[39;00m\n\u001B[1;32m     16\u001B[0m X_balanced \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([X_majority, X_minority_oversampled])\n\u001B[0;32m---> 17\u001B[0m y_balanced \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([y[y \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m], \u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloc\u001B[49m[X_minority_oversampled\u001B[38;5;241m.\u001B[39mindex]])\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(X_balanced\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "#Bootstrapping to deal with imbalance\n",
    "\n",
    "# Separate majority and minority classes\n",
    "X_majority = X[y == 0]\n",
    "X_minority = X[(y == 1) | (y==2)]\n",
    "\n",
    "# Oversample the minority class\n",
    "X_minority_oversampled = resample(\n",
    "    X_minority,\n",
    "    replace=True,\n",
    "    n_samples=len(X_majority),  # Match the size of majority class\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine majority class and oversampled minority class\n",
    "X_balanced = pd.concat([X_majority, X_minority_oversampled])\n",
    "y_balanced = pd.concat([y[y == 0], y.loc[X_minority_oversampled.index]])\n",
    "\n",
    "print(X_balanced.head(10))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[94], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Split the dataset into training and testing sets\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#10% -> 0.1 / 10 fold cross-validation\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\u001B[43mX_balanced\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchild\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfemale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msilence_before\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m, y_balanced, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m37\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Initialize logistic regression model\u001B[39;00m\n\u001B[1;32m      6\u001B[0m logistic_regression_model \u001B[38;5;241m=\u001B[39m LogisticRegression()\n",
      "\u001B[0;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "#10% -> 0.1 / 10 fold cross-validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced[['child','male','female','silence_before']], y_balanced, test_size=0.2, random_state=37)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[93], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Split the dataset into training and testing sets\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#10% -> 0.1 / 10 fold cross-validation\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\u001B[43mX_balanced\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_3\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_4\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_6\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_7\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_9\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_10\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_11\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_12\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMFCC_13\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msilence_before\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_median\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_kurtosis\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_skewness\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_q1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_q3\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_mode\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMedian_harmonicity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAvg_Pitch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mPitch_sd\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMax_harmonicity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMin_harmonicity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMedian_harmonicity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mAvg_harmonicity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSd_harmonicity\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSpectrum_energy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSpectrum_gravity_center\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSpectrum_kurtosis\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSpectrum_skewness\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchild\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfemale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmale\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m, y_balanced, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m33\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Create a logistic regression model for multinomial classification\u001B[39;00m\n\u001B[1;32m      7\u001B[0m lr_multiclass_model \u001B[38;5;241m=\u001B[39m LogisticRegression(multi_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmultinomial\u001B[39m\u001B[38;5;124m'\u001B[39m, solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msaga\u001B[39m\u001B[38;5;124m'\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000000\u001B[39m)\n",
      "\u001B[0;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "#10% -> 0.1 / 10 fold cross-validation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced[['MFCC_1','MFCC_2','MFCC_3','MFCC_4','MFCC_5','MFCC_6','MFCC_7','MFCC_8','MFCC_9','MFCC_10','MFCC_11','MFCC_12','MFCC_13','silence_before','Pitch_median','Pitch_kurtosis','Pitch_skewness','Pitch_q1','Pitch_q3','Pitch_mode','Median_harmonicity','Avg_Pitch','Pitch_sd','Max_harmonicity','Min_harmonicity','Median_harmonicity','Avg_harmonicity','Sd_harmonicity','Spectrum_energy','Spectrum_gravity_center','Spectrum_kurtosis','Spectrum_skewness','child','female','male','age']], y_balanced, test_size=0.2, random_state=33)\n",
    "\n",
    "# Create a logistic regression model for multinomial classification\n",
    "lr_multiclass_model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=10000000)\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_multiclass_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = lr_multiclass_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.710 (0.024)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a logistic regression model using k-fold cross-validation\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "# create model\n",
    "model = LogisticRegression()\n",
    "# evaluate model\n",
    "scores = cross_val_score(lr_multiclass_model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Once the features have been extracted, this is important for you to run\n",
    "\n",
    "#only harmonicity values\n",
    "X_harmonicity = X[[\"Max_harmonicity\",\"Min_harmonicity\",\"Median_harmonicity\",\"Avg_harmonicity\",\"Sd_harmonicity\"]]\n",
    "\n",
    "#only raw pitch values\n",
    "#X_pitch = X.loc[:, ~X.columns.isin([\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\",\"Max_harmonicity\",\"Min_harmonicity\",\"Median_harmonicity\",\"Avg_harmonicity\",\"Sd_harmonicity\",\"Pitch_mean\",\"Pitch_median\",\"Pitch_max\",\"Pitch_min\",\"Pitch_range\",\"Pitch_skewness\",\"Pitch_kurtosis\",\"Pitch_q1\",\"Pitch_q3\"])]\n",
    "\n",
    "#summary of pitch values\n",
    "X_summary_pitch = X[[\"Pitch_mean\",\"Pitch_sd\",\"Pitch_median\",\"Pitch_mode\",\"Pitch_q1\",\"Pitch_q3\",\"Pitch_skewness\",\"Pitch_kurtosis\",\"Pitch_max\",\"Pitch_min\",\"Pitch_range\"]]\n",
    "y_encoded_summary_pitch = y.copy()\n",
    "y_encoded_mfcc_summary_pitch = y.copy()\n",
    "\n",
    "# Find the rows containing NaN values\n",
    "index_of_rows_with_nan = np.where(X_summary_pitch.isnull().any(axis=1))[0]#X_summary_pitch[X_summary_pitch.isna().any(axis=1)]\n",
    "\n",
    "# Drop rows with specified indexes\n",
    "X_summary_pitch = X_summary_pitch.dropna()\n",
    "y_encoded_summary_pitch = np.delete(y_encoded_summary_pitch, index_of_rows_with_nan)\n",
    "\n",
    "#only MFCC coefficients\n",
    "X_mfcc = X[[\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\"]]\n",
    "\n",
    "#only MFCC coefficients + pitch summary stats\n",
    "X_mfcc_summary_pitch = X_with_filename[[\"filename\",\"Pitch_mean\",\"Pitch_sd\",\"Pitch_median\",\"Pitch_mode\",\"Pitch_q1\",\"Pitch_q3\",\"Pitch_skewness\",\"Pitch_kurtosis\",\"Pitch_max\",\"Pitch_min\",\"Pitch_range\",\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\"]]\n",
    "\n",
    "#only MFCC coefficients + pitch summary stats + silence\n",
    "X_mfcc_summary_pitch_silence = X_with_filename[[\"filename\",\"Pitch_mean\",\"Pitch_sd\",\"Pitch_median\",\"Pitch_mode\",\"Pitch_q1\",\"Pitch_q3\",\"Pitch_skewness\",\"Pitch_kurtosis\",\"Pitch_max\",\"Pitch_min\",\"Pitch_range\",\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\",\"silence_before\"]]\n",
    "\n",
    "# Find the rows containing NaN values\n",
    "index_of_rows_with_nan_2 = np.where(X_mfcc_summary_pitch.isnull().any(axis=1))[0]#X_summary_pitch[X_summary_pitch.isna().any(axis=1)]\n",
    "\n",
    "# Drop rows with specified indexes\n",
    "X_mfcc_summary_pitch = X_mfcc_summary_pitch.dropna()\n",
    "X_mfcc_summary_pitch_silence = X_mfcc_summary_pitch_silence.dropna()\n",
    "y_encoded_mfcc_summary_pitch = np.delete(y_encoded_mfcc_summary_pitch, index_of_rows_with_nan_2)\n",
    "\n",
    "\n",
    "# create model\n",
    "#Logistic regression\n",
    "classifier_lr = LogisticRegression(max_iter=10000000)\n",
    "#Support Vector Machines (SVM)\n",
    "classifier_svm = svm.SVC(kernel='rbf', C=1, max_iter=10000000)\n",
    "#decision tree\n",
    "\n",
    "\n",
    "# evaluate model\n",
    "#only mfcc\n",
    "#scores = cross_val_score(model, X_mfcc, y_encoded, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "#mfcc + summary pitch\n",
    "#scores = cross_val_score(model, X_mfcc_summary_pitch, y_encoded_mfcc_summary_pitch, scoring=['accuracy','f1'], cv=cv, n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'filename'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3652\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'filename'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#Get the group ids (user ids in this case)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m label_encoder \u001B[38;5;241m=\u001B[39m LabelEncoder()\n\u001B[0;32m----> 9\u001B[0m X_mfcc_summary_pitch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfamily_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m label_encoder\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mX_mfcc_summary_pitch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfilename\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     10\u001B[0m family_ids \u001B[38;5;241m=\u001B[39m X_mfcc_summary_pitch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfamily_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m)\n\u001B[1;32m     12\u001B[0m X_mfcc_summary_pitch\u001B[38;5;241m=\u001B[39mX_mfcc_summary_pitch\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3760\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3761\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3762\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3763\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/miniconda3/envs/pyannote/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3653\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3654\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3655\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3656\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3657\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3658\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3659\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'filename'"
     ]
    }
   ],
   "source": [
    "# Specify the metrics you want to evaluate\n",
    "scoring = ['balanced_accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted','roc_auc_ovr_weighted']\n",
    "\n",
    "n_partitions = 5\n",
    "group_kfold = GroupKFold(n_splits=n_partitions)\n",
    "\n",
    "#Get the group ids (user ids in this case)\n",
    "label_encoder = LabelEncoder()\n",
    "X_mfcc_summary_pitch['family_id'] = label_encoder.fit_transform(X_mfcc_summary_pitch['filename'])\n",
    "family_ids = X_mfcc_summary_pitch['family_id'].astype(str)\n",
    "\n",
    "X_mfcc_summary_pitch=X_mfcc_summary_pitch.drop(columns=['filename'])\n",
    "\n",
    "#now we need to remove filnemae from\n",
    "\n",
    "# Perform cross-validation\n",
    "#only mfcc\n",
    "#cv_results = cross_validate(classifier_lr, X_mfcc, y_encoded, scoring=scoring, cv=cv_partitions)\n",
    "#mfcc + summary pitch stats\n",
    "cv_results = cross_validate(classifier_lr, X_mfcc_summary_pitch_silence, y_encoded_mfcc_summary_pitch, scoring=scoring, cv=group_kfold,groups=family_ids)\n",
    "\n",
    "# report performance\n",
    "# Print the results\n",
    "for score in scoring:\n",
    "    print(f\"{score}: {cv_results[f'test_{score}'].mean()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'family_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m n_partitions \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m     14\u001B[0m group_kfold \u001B[38;5;241m=\u001B[39m GroupKFold(n_splits\u001B[38;5;241m=\u001B[39mn_partitions)\n\u001B[0;32m---> 16\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m cross_validate(nn_multiclass_model , X_mfcc_summary_pitch, y_encoded_mfcc_summary_pitch, scoring\u001B[38;5;241m=\u001B[39mscoring, cv\u001B[38;5;241m=\u001B[39mgroup_kfold,groups\u001B[38;5;241m=\u001B[39m\u001B[43mfamily_ids\u001B[49m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# report performance\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Print the results\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m score \u001B[38;5;129;01min\u001B[39;00m scoring:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'family_ids' is not defined"
     ]
    }
   ],
   "source": [
    "#Multiclass evaluation with LR, Decision Tree and Random Forest\n",
    "\n",
    "# Create a logistic regression model for multinomial classification\n",
    "lr_multiclass_model = LogisticRegression(multi_class='multinomial', solver='newton-cg', max_iter=1000000)\n",
    "# Initialize the DecisionTreeClassifier\n",
    "dt_multiclass_model = DecisionTreeClassifier(random_state=43)\n",
    "rf_multiclass_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "nn_multiclass_model = MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, random_state=42)\n",
    "\n",
    "# Specify the metrics you want to evaluate\n",
    "scoring = ['balanced_accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted','roc_auc_ovr_weighted']\n",
    "\n",
    "n_partitions = 5\n",
    "group_kfold = GroupKFold(n_splits=n_partitions)\n",
    "\n",
    "cv_results = cross_validate(nn_multiclass_model , X_mfcc_summary_pitch, y_encoded_mfcc_summary_pitch, scoring=scoring, cv=group_kfold,groups=family_ids)\n",
    "\n",
    "# report performance\n",
    "# Print the results\n",
    "for score in scoring:\n",
    "    print(f\"{score}: {cv_results[f'test_{score}'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.671 (0.019)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='rbf', C=1, max_iter=10000000)\n",
    "scores = cross_val_score(clf, X_mfcc, y, cv=5, scoring='accuracy')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-6.54160229]\n",
      "Coefficients: [[ 2.54374108e-02 -3.95437264e-03  8.00920613e-03 -1.67677412e-04\n",
      "  -2.62352527e-03 -5.74395977e-03 -1.78084678e-01 -3.09817129e-02\n",
      "   1.01960052e-03  1.46068377e-03 -4.41083248e-04 -1.62915595e-03\n",
      "  -8.03525581e-03  1.31612314e-02 -1.32334237e-02  2.11545866e-02\n",
      "  -2.68567960e-02 -1.26484462e-02  4.80891870e-02 -3.51550436e-02\n",
      "   9.89322826e-03  2.10876374e-02 -1.79160030e-02  2.17946972e-03]]\n",
      "Regularization Strength (C): 1.0\n",
      "Number of Iterations: [1167]\n",
      "Classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "#Train LR model based on all the data\n",
    "all_data_lr = classifier_lr.fit(X_mfcc_summary_pitch.drop(columns=['family_id']), y_encoded_mfcc_summary_pitch)\n",
    "\n",
    "# Details about the LR model\n",
    "# Inspect the intercept and coefficients\n",
    "print(\"Intercept:\", all_data_lr.intercept_)\n",
    "print(\"Coefficients:\", all_data_lr.coef_)\n",
    "\n",
    "# Explore other attributes\n",
    "print(\"Regularization Strength (C):\", all_data_lr.C)\n",
    "print(\"Number of Iterations:\", all_data_lr.n_iter_)\n",
    "print(\"Classes:\", all_data_lr.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path '../EMBRACE-data-analysis/data/summer23-pitt/Family_32/32_Chromebook Data/32_voice-recordings/voice-recording-1687549850446-1687550006814_no_silences' exists.\n",
      "Total custom-size segments: 125\n",
      "1.0\n",
      "1.0\n",
      "0.6662500000000007\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.7031250000000018\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.2687500000000007\n",
      "1.0\n",
      "0.23187499999999872\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.281875000000003\n",
      "0.5062499999999943\n",
      "1.0\n",
      "0.2993749999999977\n",
      "1.0\n",
      "0.23187499999999517\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.12562499999999943\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.7587499999999991\n",
      "1.0\n",
      "0.33312500000000256\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.5900000000000034\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.25687499999999375\n",
      "1.000000000000007\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.4043749999999875\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.8887500000000017\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.07125000000000625\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.5168750000000131\n",
      "0.3881250000000023\n",
      "1.0\n",
      "0.5356250000000102\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.948750000000004\n",
      "1.0\n",
      "0.13062499999999488\n",
      "0.9112499999999955\n",
      "1.0\n",
      "0.012500000000002842\n",
      "0.21937499999999943\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9274999999999949\n",
      "0.7425000000000068\n",
      "1.0\n",
      "1.0\n",
      "0.025000000000034106\n",
      "1.0\n",
      "1.0\n",
      "0.4300000000000068\n",
      "0.8100000000000023\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.2906250000000341\n",
      "0.7425000000000068\n",
      "0.8943749999999682\n",
      "1.0\n",
      "0.4006249999999909\n",
      "0        1.290938\n",
      "1        2.290938\n",
      "2        3.290938\n",
      "3        4.497188\n",
      "4        5.497188\n",
      "          ...    \n",
      "117    142.210312\n",
      "118    143.192813\n",
      "119    144.188438\n",
      "120    145.420313\n",
      "121    146.420313\n",
      "Name: start, Length: 122, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Use the LR model to predict unseen audios\n",
    "#test_filename = '../EMBRACE-data-analysis/audios/asu/par007_record-673758082.558921.wav'\n",
    "test_filename = '../EMBRACE-data-analysis/data/summer23-pitt/Family_32/32_Chromebook Data/32_voice-recordings/voice-recording-1687549850446-1687550006814.mp3'\n",
    "#test_filename = \"../EMBRACE-data-analysis/data/summer23-pitt/Family_31/31_Chromebook Data/31_voice-recordings/voice-recording-1684007087987-1684007165740.mp3\"\n",
    "\n",
    "current_test_df = pd.DataFrame()\n",
    "current_test_df = add_labeled_data_no_labels(current_test_df,test_filename)\n",
    "\n",
    "#Print how many rows and columns has the matrix that represent the whole audio\n",
    "#print(current_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[52], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;241m3\u001B[39m\u001B[38;5;66;03m#use the test data to predict the labels\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mcurrent_test_df\u001B[49m[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m5\u001B[39m])\n\u001B[1;32m      4\u001B[0m current_test_df\u001B[38;5;241m=\u001B[39mcurrent_test_df\u001B[38;5;241m.\u001B[39mdropna()\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m current_test_df \u001B[38;5;241m=\u001B[39m add_pitch_summary_stats(current_test_df,\u001B[38;5;241m50\u001B[39m,\u001B[38;5;241m550\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'current_test_df' is not defined"
     ]
    }
   ],
   "source": [
    "#use the test data to predict the labels\n",
    "\n",
    "print(current_test_df[0:5])\n",
    "current_test_df=current_test_df.dropna().reset_index(drop=True)\n",
    "\n",
    "current_test_df = add_pitch_summary_stats(current_test_df,50,550)\n",
    "#print(current_test_df)\n",
    "# Make predictions on the training data\n",
    "#only MFCC coefficients + pitch summary stats\n",
    "X_mfcc_summary_pitch_test = current_test_df[[\"Pitch_mean\",\"Pitch_sd\",\"Pitch_median\",\"Pitch_mode\",\"Pitch_q1\",\"Pitch_q3\",\"Pitch_skewness\",\"Pitch_kurtosis\",\"Pitch_max\",\"Pitch_min\",\"Pitch_range\",\"MFCC_1\",\"MFCC_2\",\"MFCC_3\",\"MFCC_4\",\"MFCC_5\",\"MFCC_6\",\"MFCC_7\",\"MFCC_8\",\"MFCC_9\",\"MFCC_10\",\"MFCC_11\",\"MFCC_12\",\"MFCC_13\"]]\n",
    "\n",
    "#print(X_mfcc_summary_pitch_test)\n",
    "#print(current_test_df.columns)\n",
    "\n",
    "predictions = all_data_lr.predict(X_mfcc_summary_pitch_test)\n",
    "#print(predictions)\n",
    "#print(sum(predictions))\n",
    "\n",
    "predictions_as_df = pd.DataFrame(predictions)\n",
    "predictions_as_df.rename(columns={0:'prediction'},inplace=True)\n",
    "\n",
    "#Concatenate start end from the audio and the predicted label to make manual inspection\n",
    "df_check_prediction_values = pd.concat([current_test_df[[\"start\",\"end\"]],predictions_as_df],axis=1)\n",
    "print(df_check_prediction_values[70:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predicted_labels_audio(test_filename,current_test_df,predictions):\n",
    "    #Testing if the predictions were good or not\n",
    "    #add the prediction label to the X\n",
    "    current_test_df['prediction']=predictions\n",
    "\n",
    "    #Load audio\n",
    "    snd = parselmouth.Sound(test_filename)\n",
    "\n",
    "    #https://www.geeksforgeeks.org/python-basic-gantt-chart-using-matplotlib/\n",
    "    # Declaring a figure \"gnt\"\n",
    "    fig, gnt = plt.subplots()\n",
    "\n",
    "    # Setting Y-axis limits\n",
    "    gnt.set_ylim(0, 30)\n",
    "\n",
    "    # Setting X-axis limits\n",
    "    gnt.set_xlim(0, snd.duration)\n",
    "\n",
    "    # Setting labels for x-axis and y-axis\n",
    "    gnt.set_xlabel('Time [s]')\n",
    "    gnt.set_ylabel('Speaker')\n",
    "\n",
    "    # Setting ticks on y-axis\n",
    "    gnt.set_yticks([5,15])\n",
    "    # Labelling tickes of y-axis\n",
    "    gnt.set_yticklabels(['Child',\"Adult\"])\n",
    "\n",
    "    # Setting graph attribute\n",
    "    gnt.grid(True)\n",
    "\n",
    "    tuples_pitch_times_child=[]\n",
    "    tuples_pitch_times_adult=[]\n",
    "    for index in current_test_df.index:\n",
    "        #Prediction of 1 means child, prediction of 0 means adult\n",
    "        if(current_test_df.loc[index,'prediction']==1):\n",
    "            tuples_pitch_times_child.append([current_test_df.loc[index,'start'],current_test_df.loc[index,'end']])\n",
    "        else:\n",
    "            tuples_pitch_times_adult.append([current_test_df.loc[index,'start'],current_test_df.loc[index,'end']])\n",
    "\n",
    "    #get data in the format that the plotter requires (second_start, duration_span)\n",
    "    timespan_data_child = []\n",
    "    for i in range(0,len(tuples_pitch_times_child)):\n",
    "        time_tuple = tuples_pitch_times_child[i]\n",
    "        plot_format_tuple = (time_tuple[0],time_tuple[1]-time_tuple[0])\n",
    "        timespan_data_child.append(plot_format_tuple)\n",
    "\n",
    "    timespan_data_adult = []\n",
    "    for i in range(0,len(tuples_pitch_times_adult)):\n",
    "        time_tuple = tuples_pitch_times_adult[i]\n",
    "        plot_format_tuple = (time_tuple[0],time_tuple[1]-time_tuple[0])\n",
    "        timespan_data_adult.append(plot_format_tuple)\n",
    "\n",
    "    # Declaring multiple bars in at same level and same width\n",
    "    gnt.broken_barh(timespan_data_child, (5, 9),\n",
    "                             facecolors ='tab:orange')\n",
    "    gnt.broken_barh(timespan_data_adult, (15, 9),\n",
    "                             facecolors ='tab:blue')\n",
    "\n",
    "    #Get proportion of time where child talked and where parent talked\n",
    "    total_segments_child = sum(current_test_df['prediction'])\n",
    "    total_segments_adult = len(current_test_df) - total_segments_child\n",
    "    proportion_child_participation = total_segments_child/len(current_test_df)\n",
    "    proportion_adult_participation = 1 - proportion_child_participation\n",
    "    print(\"Child participation \"+str(round(proportion_child_participation*100,2))+\"%\")\n",
    "    print(\"Adult participation \"+str(round(proportion_adult_participation*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child participation 26.09%\n",
      "Adult participation 73.91%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "analyze_predicted_labels_audio(test_filename,current_test_df,predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
