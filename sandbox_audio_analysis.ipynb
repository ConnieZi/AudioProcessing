{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parselmouth\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "#import set of functions I made for the automatic analysis of the audios\n",
    "import sys\n",
    "\n",
    "#here the path needs to be changed to your own local path\n",
    "sys.path.insert(1, '/Users/jab464/Documents/Github/EMBRACE-data-analysis/')\n",
    "from audio_analysis_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#Audio ASU mom-child\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par007_record-673758082.558921.wav'\n",
    "\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par007_record-673758082.558921_reduced_noise.wav'\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/mom-child-62'\n",
    "\n",
    "#Audio dad-child (Pittsburgh)\n",
    "#filename='../EMBRACE-data-analysis/audios/record-667269360.9571331.wav'\n",
    "#filename='../EMBRACE-data-analysis/audios/record-667269360.9571331_reduced_noise.wav'\n",
    "\n",
    "#Audio mom-grandma-child (Pittsburgh)\n",
    "#filename = '../EMBRACE-data-analysis/audios/record-672279722.51811.wav'\n",
    "\n",
    "#ASU par001\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par001-record-656468528.8997459_silence_removed.wav'\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par001-record-656468528.8997459.wav'\n",
    "\n",
    "#ASU par010 (with Sindhu there) DO NOT USE\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par010-record-691979477.209939.wav'\n",
    "\n",
    "#ASU par102\n",
    "#filename = \"../EMBRACE-data-analysis/audios/asu/par102-record-666060012.245397.wav\"\n",
    "\n",
    "#ASU par003\n",
    "#filename = \"../EMBRACE-data-analysis/audios/asu/par003-record-674009351.193422.wav\"\n",
    "\n",
    "#short test audio\n",
    "filename = \"../EMBRACE-data-analysis/data/two_speakers.wav\"\n",
    "\n",
    "#Pitt 2023 study with families\n",
    "#filename = \"../EMBRACE-data-analysis/audios/summer23-pitt/18_voice-recordings/voice-recording-1684538821337-1684538952706.mp3\"\n",
    "\n",
    "#overlap test audio\n",
    "filename_overlap = \"../EMBRACE-data-analysis/audios/overlap.wav\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00:00:00.126 -->  00:00:06.471]\n",
      "[ 00:00:06.724 -->  00:00:08.952]\n",
      "[ 00:00:09.289 -->  00:00:11.770]\n",
      "(0.12656250000000002, 6.471562500000001)\n",
      "Duration segment 0: 6.345000000000001 secs.\n",
      "(6.7246875, 8.9521875)\n",
      "Duration segment 1: 2.227500000000001 secs.\n",
      "(9.2896875, 11.770312500000003)\n",
      "Duration segment 2: 2.4806250000000034 secs.\n",
      "Total time active voice: 11.053125000000005 secs.\n"
     ]
    }
   ],
   "source": [
    "#Get active voice segments\n",
    "active_voice_segments = get_voice_activity(filename,True,False,True)\n",
    "\n",
    "#Calculate total time voice\n",
    "total_time_voice = 0\n",
    "counter = 0\n",
    "for segment in active_voice_segments:\n",
    "    start = segment[0]\n",
    "    end = segment[1]\n",
    "    time_duration = end-start\n",
    "    print(segment)\n",
    "    print(\"Duration segment \"+str(counter)+\": \"+str(time_duration)+\" secs.\")\n",
    "    total_time_voice = total_time_voice + time_duration\n",
    "    counter = counter + 1\n",
    "print(\"Total time active voice: \"+str(total_time_voice)+\" secs.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set() # Use seaborn's default style to make attractive graphs\n",
    "plt.rcParams['figure.dpi'] = 200 # Show nicely large images in this notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load audio\n",
    "snd = parselmouth.Sound(filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(snd.xs(), snd.values.T)\n",
    "plt.xlim([snd.xmin, snd.xmax])\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"amplitude\")\n",
    "plt.show() # or plt.savefig(\"sound.png\"), or plt.savefig(\"sound.pdf\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Remove noise from the audio\n",
    "#snd = parselmouth.praat.call(snd, \"Remove noise\", 0.0, 0.0, 0.025, 80.0, 10000.0, 40.0, \"Spectral subtraction\")\n",
    "\n",
    "pitch = snd.to_pitch()\n",
    "pitch_values = pitch.selected_array['frequency']\n",
    "pitch_values[pitch_values==0] = np.nan\n",
    "plt.plot(pitch.xs(), pitch_values, 'o', markersize=2)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#MFCC coefficients\n",
    "mfcc_coefs = snd.to_mfcc()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Classify small chunks into the identity of the speaker\n",
    "audio = AudioSegment.from_wav(filename)\n",
    "\n",
    "voice_activity = get_voice_activity(filename,False,False,False)\n",
    "custom_size_voice_activity = custom_size_segments(voice_activity)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#manual labeling of the voice segments\n",
    "#child=c\n",
    "#robot male english = rme\n",
    "#robot male spanish = rms\n",
    "#robot female english = rfe\n",
    "#robot female spanish = rfs\n",
    "#adult male = am\n",
    "#adult female = af\n",
    "#(if additional adults number them)\n",
    "#adult male 2 = am2\n",
    "#adult female 2 = af2 ...\n",
    "#if they are talking simultaneously = A&B\n",
    "#if they are talking separately but sequentially = A,B\n",
    "\n",
    "labeling_file_str = \"\"\n",
    "starting_index = int(input(\"Which index would you like to start from? (type 0 if from the beginning or >0 if you want to continue labeling):\"))\n",
    "for i in range (starting_index,len(custom_size_voice_activity)):\n",
    "    active_segment = custom_size_voice_activity[i]\n",
    "#for active_segment in custom_size_voice_activity:\n",
    "    play_audio_segment(audio,active_segment[0]*1000,active_segment[1]*1000)\n",
    "    label = input(\"Enter label for this audio segment: \")\n",
    "    if(label==\"end\"):#stop tagging\n",
    "        break\n",
    "    while(label==\"r\"):#if you want the audio chunk to be repeated\n",
    "        play_audio_segment(audio,active_segment[0]*1000,active_segment[1]*1000)\n",
    "        label = input(\"Enter label for this audio segment: \")\n",
    "        if(label==\"end\"):\n",
    "            break\n",
    "    label_line = str(i)+\" \"+str(active_segment)+\" \"+label\n",
    "    labeling_file_str = labeling_file_str + label_line + \"\\n\"\n",
    "    print(str(i)+\" out of \"+str(len(custom_size_voice_activity)-1))\n",
    "if(starting_index==0):\n",
    "    f = open(filename+\"_labeled.txt\", \"w\")\n",
    "    f.write(labeling_file_str)\n",
    "    f.close()\n",
    "elif(starting_index<len(custom_size_voice_activity)):\n",
    "    f = open(filename+\"_labeled.txt\", \"a\")\n",
    "    f.write(labeling_file_str)\n",
    "    f.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#for active_segment in custom_size_voice_activity:\n",
    "#first, get the labels from the file\n",
    "labels = []\n",
    "with open(filename+\"_labeled.txt\") as file:\n",
    "  labels=file.readlines()\n",
    "\n",
    "correct_predictions = 0\n",
    "correct_predictions_single_label = 0\n",
    "total_single_labels = 0\n",
    "for i in range(0,len(custom_size_voice_activity)):\n",
    "    label_str = labels[i]\n",
    "    label_str = label_str.replace(\", \",\",\")\n",
    "    label_info = label_str.split(\" \")\n",
    "    label = label_info[2]\n",
    "    active_segment = custom_size_voice_activity[i]\n",
    "    #print(active_segment)\n",
    "    #predicted_label_str = classify_audio_freq(avg_pitch(pitch,70,400,[active_segment]))\n",
    "    predicted_label_str = classify_audio_freq(median_pitch(pitch,60,500,active_segment))\n",
    "    q3 = quantile_pitch(pitch,0,1000,active_segment,75)\n",
    "    q1 = quantile_pitch(pitch,0,1000,active_segment,25)\n",
    "    iqr = q3-q1\n",
    "\n",
    "    tagged_label = label[:-1]\n",
    "    predicted_labels = predicted_label_str.split(\"|\")\n",
    "    correct_prediction = False\n",
    "    if('&' not in tagged_label) and (',' not in tagged_label):\n",
    "        total_single_labels = total_single_labels + 1\n",
    "    for predicted_label in predicted_labels:\n",
    "        if predicted_label in tagged_label:\n",
    "            correct_prediction = True\n",
    "            break\n",
    "    if correct_prediction:\n",
    "        correct_predictions = correct_predictions+1\n",
    "        if('&' not in tagged_label) and (',' not in tagged_label):\n",
    "            correct_predictions_single_label = correct_predictions_single_label + 1\n",
    "    else:\n",
    "        print(\"iqr: \"+str(iqr)+\" q1: \"+str(q1)+\" q3: \"+str(q3))\n",
    "        print(str(i)+\" \"+tagged_label+\" -> \"+predicted_label_str+ \" | Prediction: \"+str(correct_prediction))\n",
    "\n",
    "avg_accuracy = correct_predictions/len(custom_size_voice_activity)\n",
    "avg_accuracy_single_labels = correct_predictions_single_label/total_single_labels\n",
    "print(\"Avg accuracy: \"+str(avg_accuracy*100)+\"%\")\n",
    "print(\"Avg single labels: \"+str(avg_accuracy_single_labels*100)+\"%\")\n",
    "\n",
    "#play(audio)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play_and_plot_audio_segment_by_index(audio,pitch,custom_size_voice_activity,67)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The average man’s speaking voice, for example, typically has a fundamental frequency between 85 Hz and 155 Hz. A woman’s speech range is about 165 Hz to 255 Hz, and a child’s voice typically ranges from 250 Hz to 300 Hz and higher.\n",
    "#https://www.axiomaudio.com/blog/audio-oddities-frequency-ranges-of-male-female-and-childrens-voices#:~:text=A%20woman's%20speech%20range%20is,to%20300%20Hz%20and%20higher.\n",
    "#for adult male\n",
    "#secs_pitch_adult,pitch_values_adult,index_pitch_values_adult = extract_pitch_segments(pitch,85,155)\n",
    "\n",
    "#for adult female\n",
    "secs_pitch_adult,pitch_values_adult,index_pitch_values_adult = extract_pitch_segments(pitch,165,255)\n",
    "\n",
    "#for the child\n",
    "secs_pitch_child,pitch_values_child,index_pitch_values_child = extract_pitch_segments(pitch,250,400)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Generation of the time intervals where there is a significant participation of the child (>= .3 second)\n",
    "threshold_time_interval = 1\n",
    "tuples_pitch_times_child = []\n",
    "prev_sec = secs_pitch_child[0]\n",
    "curr_time_tuple = (secs_pitch_child[0],)\n",
    "for i in range(1,len(secs_pitch_child)):\n",
    "    next_sec = secs_pitch_child[i]\n",
    "    time_diff = next_sec - prev_sec\n",
    "    if time_diff>threshold_time_interval or i==(len(secs_pitch_child)-1):\n",
    "        curr_time_tuple+=(prev_sec,)\n",
    "        tuples_pitch_times_child.append(curr_time_tuple)\n",
    "        curr_time_tuple = (next_sec,)\n",
    "    prev_sec = next_sec\n",
    "secs_pitch_child\n",
    "tuples_pitch_times_child"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Generation of the time intervals where there is a significant participation of the adult (>= 1 second)\n",
    "threshold_time_interval = 1\n",
    "tuples_pitch_times_adult = []\n",
    "tuples_pitch_indexes_adult = []\n",
    "prev_sec = secs_pitch_adult[0]\n",
    "prev_index = 0\n",
    "curr_time_tuple = (secs_pitch_adult[0],)\n",
    "curr_index_tuple = (0,)\n",
    "for i in range(1,len(secs_pitch_adult)):\n",
    "    next_sec = secs_pitch_adult[i]\n",
    "    next_index = i\n",
    "    time_diff = next_sec - prev_sec\n",
    "    if(time_diff>threshold_time_interval):\n",
    "        curr_time_tuple+=(prev_sec,)\n",
    "        curr_index_tuple+=(prev_index,)\n",
    "        tuples_pitch_times_adult.append(curr_time_tuple)\n",
    "        tuples_pitch_indexes_adult.append(curr_index_tuple)\n",
    "        curr_time_tuple = (next_sec,)\n",
    "        curr_index_tuple = (next_index,)\n",
    "    prev_sec = next_sec\n",
    "    prev_index = next_index\n",
    "tuples_pitch_times_adult"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Potential false positives (child segments that are detected as adult)\n",
    "tuples_pitch_times_adult_but_child = []\n",
    "\n",
    "#Indexes that should be removed\n",
    "indexes_to_remove = []\n",
    "\n",
    "#checks avg pitch for each segment\n",
    "for i in range(0,len(tuples_pitch_indexes_adult)):\n",
    "    curr_index_tuple = tuples_pitch_indexes_adult[i]\n",
    "    start_index = curr_index_tuple[0]\n",
    "    end_index = curr_index_tuple[1]\n",
    "    sum = 0\n",
    "    for j in range(start_index,end_index+1):\n",
    "        sum = sum + pitch_values_adult[j]\n",
    "    n_frames = end_index-start_index\n",
    "    avg=sum/n_frames\n",
    "    if(avg>=240):\n",
    "        tuples_pitch_times_adult_but_child.append(tuples_pitch_times_adult[i])\n",
    "        indexes_to_remove.append(i)\n",
    "tuples_pitch_times_adult_but_child"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/python-basic-gantt-chart-using-matplotlib/\n",
    "# Declaring a figure \"gnt\"\n",
    "fig, gnt = plt.subplots()\n",
    "\n",
    "# Setting Y-axis limits\n",
    "gnt.set_ylim(0, 30)\n",
    "\n",
    "# Setting X-axis limits\n",
    "gnt.set_xlim(0, snd.duration)\n",
    "\n",
    "# Setting labels for x-axis and y-axis\n",
    "gnt.set_xlabel('Time [s]')\n",
    "gnt.set_ylabel('Speaker')\n",
    "\n",
    "# Setting ticks on y-axis\n",
    "gnt.set_yticks([5,15])\n",
    "# Labelling tickes of y-axis\n",
    "gnt.set_yticklabels(['Child',\"Adult\"])\n",
    "\n",
    "# Setting graph attribute\n",
    "gnt.grid(True)\n",
    "\n",
    "#get data in the format that the plotter requires (second_start, duration_span)\n",
    "timespan_data_child = []\n",
    "for i in range(0,len(tuples_pitch_times_child)):\n",
    "    time_tuple = tuples_pitch_times_child[i]\n",
    "    plot_format_tuple = (time_tuple[0],time_tuple[1]-time_tuple[0])\n",
    "    timespan_data_child.append(plot_format_tuple)\n",
    "\n",
    "timespan_data_adult = []\n",
    "for i in range(0,len(tuples_pitch_times_adult)):\n",
    "    time_tuple = tuples_pitch_times_adult[i]\n",
    "    plot_format_tuple = (time_tuple[0],time_tuple[1]-time_tuple[0])\n",
    "    timespan_data_adult.append(plot_format_tuple)\n",
    "\n",
    "# Declaring multiple bars in at same level and same width\n",
    "gnt.broken_barh(timespan_data_child, (5, 9),\n",
    "                         facecolors ='tab:orange')\n",
    "gnt.broken_barh(timespan_data_adult, (15, 9),\n",
    "                         facecolors ='tab:blue')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load audio for slicing purposes\n",
    "audio = AudioSegment.from_wav(filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Play adult segments\n",
    "#play_audio_segment(audio,274760,277030)\n",
    "#play_audio_segment(audio,99690, 100980)\n",
    "#play_audio_segment(audio,17430,18530)\n",
    "#play_audio_segment(audio,274760,277730)\n",
    "\n",
    "#dad-child audios\n",
    "play_audio_segment(audio,85242,88042)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Play child segments\n",
    "#play_audio_segment(audio,98860,100200)\n",
    "#play_audio_segment(audio,99700,100980)#wrongly labeled as adult\n",
    "#play_audio_segment(audio,295110,295600)#wrongly labeled as adult\n",
    "#play_audio_segment(audio,188960,189600)#wrongly labeled as adult\n",
    "#play_audio_segment(audio,88330,89110)#wrongly labeled as adult\n",
    "#play_audio_segment(audio,294420,296090)#wrongly labeled as adult\n",
    "\n",
    "#dad-child audios\n",
    "play_audio_segment(audio,137822,139842)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Play all the child segments\n",
    "print(len(tuples_pitch_times_child))\n",
    "for i in range(5,10):#\n",
    "    time_tuple = tuples_pitch_times_child[i]\n",
    "    print(time_tuple)\n",
    "    play_audio_segment(audio,round(time_tuple[0]*1000),round(time_tuple[1]*1000))\n",
    "    time.sleep(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Play all the adult segments\n",
    "for i in range(15,20):#len(tuples_pitch_times_adult)):\n",
    "    time_tuple = tuples_pitch_times_adult[i]\n",
    "    print(time_tuple)\n",
    "    play_audio_segment(audio,time_tuple[0]*1000,time_tuple[1]*1000)\n",
    "    time.sleep(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Play all the adult segments that should be child\n",
    "for i in range(0,1):#len(tuples_pitch_times_adult_but_child)):\n",
    "    time_tuple = tuples_pitch_times_adult_but_child[i]\n",
    "    print(time_tuple)\n",
    "    play_audio_segment(audio,round(time_tuple[0]*1000),round(time_tuple[1]*1000))\n",
    "    time.sleep(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Segment the audio in smaller chunks\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "audio_to_chunk = AudioSegment.from_file(filename , \"wav\")\n",
    "chunk_length_ms = 2000 # pydub calculates in millisec\n",
    "chunks = make_chunks(audio_to_chunk, chunk_length_ms) #Make chunks of one sec\n",
    "\n",
    "#Export all of the individual chunks as wav files\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_name = \"../EMBRACE-data-analysis/audios/asu/mom-child-{0}.wav\".format(i)\n",
    "    print(\"exporting\", chunk_name)\n",
    "    chunk.export(chunk_name, format=\"wav\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Speaker diarization with pyannote\n",
    "pipeline_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
    "                                    use_auth_token=\"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\")\n",
    "\n",
    "# apply the pipeline to an audio file\n",
    "diarization = pipeline_diarization(filename)\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(\"diarization.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Overlap detection with pyannote\n",
    "pipeline_overlap = Pipeline.from_pretrained(\"pyannote/overlapped-speech-detection\",\n",
    "                                    use_auth_token=\"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\")\n",
    "\n",
    "#filename_overlap = \"../EMBRACE-data-analysis/dad-child-14.wav\"\n",
    "#filename_overlap = \"../EMBRACE-data-analysis/mom-grandmom-child-28.wav\"\n",
    "filename_overlap = \"../EMBRACE-data-analysis/mom-child-20.wav\"\n",
    "\n",
    "output = pipeline_overlap(filename_overlap)\n",
    "\n",
    "audio = AudioSegment.from_wav(filename_overlap)\n",
    "\n",
    "#mom-grandmom-child: 9\n",
    "for speech in output.get_timeline().support():\n",
    "    # two or more speakers are active between speech.start and speech.end\n",
    "    print(speech)\n",
    "    play_audio_segment(audio,speech.start*1000,speech.end*1000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#inter agreement analysis\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#read first two files\n",
    "f_coder1 = open(\"../EMBRACE-data-analysis/audios/asu/PAR001_Record-656468528.8997459_silence_removed-Coded.txt\", \"r\")\n",
    "f_coder2 = open(\"../EMBRACE-data-analysis/audios/asu/par001-record-656468528.8997459.wav_labeled_Jordan.txt\", \"r\")\n",
    "\n",
    "coder1_raw_list = f_coder1.readlines()\n",
    "coder2_raw_list = f_coder2.readlines()\n",
    "\n",
    "#code lists\n",
    "coder1_list = []\n",
    "coder2_list = []\n",
    "\n",
    "for line in coder1_raw_list:\n",
    "    code = line.split('\\t')[-1]\n",
    "    coder1_list.append(code[:-1])\n",
    "\n",
    "for line in coder2_raw_list:\n",
    "    code = line.split(' ')[-1]\n",
    "    coder2_list.append(code[:-1])\n",
    "\n",
    "print(len(coder1_list))\n",
    "print(len(coder2_list))\n",
    "\n",
    "cohen_kappa_score(coder1_list, coder2_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Audio ASU mom-child\n",
    "filename1 = '../EMBRACE-data-analysis/data/asu/par007_record-673758082.558921.wav'\n",
    "\n",
    "#Audio dad-child (Pittsburgh)\n",
    "#filename='../EMBRACE-data-analysis/audios/record-667269360.9571331.wav'\n",
    "#filename='../EMBRACE-data-analysis/audios/record-667269360.9571331_reduced_noise.wav'\n",
    "\n",
    "#ASU par001\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par001-record-656468528.8997459_silence_removed.wav'\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par001-record-656468528.8997459.wav'\n",
    "\n",
    "#ASU par010 (with Sindhu there)\n",
    "#filename = '../EMBRACE-data-analysis/audios/asu/par010-record-691979477.209939.wav'\n",
    "\n",
    "#ASU par102\n",
    "filename2 = \"../EMBRACE-data-analysis/data/asu/par102-record-666060012.245397.wav\"\n",
    "\n",
    "#ASU par003\n",
    "filename3 = \"../EMBRACE-data-analysis/data/asu/par003-record-674009351.193422.wav\"\n",
    "\n",
    "#ASU par001\n",
    "filename4 = '../EMBRACE-data-analysis/data/asu/par001-record-656468528.8997459.wav'\n",
    "\n",
    "filenames = [filename1,filename2,filename3,filename4]\n",
    "sound_objects = []\n",
    "segments_data = []\n",
    "#Get active voice segments\n",
    "active_voice_segments = []\n",
    "segment_index = 0\n",
    "for i in range(0,len(filenames)):#len(filenames)):\n",
    "    active_voice_segments.append(get_voice_activity(filenames[i],True,False,True))\n",
    "    segments_voice_activity = custom_size_segments(active_voice_segments[i])\n",
    "\n",
    "    #Load audio\n",
    "    sound_objects.append(parselmouth.Sound(filenames[i]))\n",
    "\n",
    "    #Get pitch values\n",
    "    curr_pitch = sound_objects[i].to_pitch()\n",
    "\n",
    "    #Get mfcc coefs\n",
    "    curr_mfcc_coefs = sound_objects[i].to_mfcc()\n",
    "\n",
    "    pitch_values = curr_pitch.selected_array['frequency']\n",
    "\n",
    "\n",
    "    for j in range(0,len(segments_voice_activity)):\n",
    "        print(segments_voice_activity[j])\n",
    "        segments_data.append([])\n",
    "        active_segment = segments_voice_activity[j]\n",
    "        #print(active_segment)\n",
    "        #predicted_label_str = classify_audio_freq(avg_pitch(pitch,70,400,[active_segment]))\n",
    "        pitch_values_active_segment = extract_pitch_values_segment(curr_pitch,active_segment)\n",
    "        segments_data[segment_index].append(pitch_values_active_segment)\n",
    "        segment_index = segment_index + 1\n",
    "        print(len(pitch_values_active_segment))\n",
    "\n",
    "    #print(len(curr_pitch))\n",
    "    #print(len(curr_mfcc_coefs))\n",
    "    #numeric_data_audios[i].append(curr_pitch)\n",
    "    #numeric_data_audios[i].append(curr_mfcc_coefs)\n",
    "\n",
    "    #print(len(numeric_data_audios[i]))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
